# -*- coding: utf-8 -*-
"""Drug_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XoyJujChWxCSUcp_L9Re7ec98U6gCLmA
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
db=pd.read_csv("/content/drive/MyDrive/Collab Dataset/Drug Detection/dev_data_translated (1).csv")

print(db.head())

import pandas as pd

# Load datasets
train_path = "/content/drive/MyDrive/Collab Dataset/Drug Detection/train_data_translated (1).csv"
dev_path = "/content/drive/MyDrive/Collab Dataset/Drug Detection/dev_data_translated (1).csv"

train_df = pd.read_csv(train_path)
dev_df = pd.read_csv(dev_path)

# Display first few rows
print("Train Data Sample:")
display(train_df.head())

print("Dev Data Sample:")
display(dev_df.head())

def preprocess_text(text):
    if pd.isna(text):
        return ""
    text = text.lower()
    text = re.sub(r'http\S+|www\S+', '', text)  # Remove URLs
    text = re.sub(r'@\w+', '', text)  # Remove mentions
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special chars
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab') # Download the Punkt Tokenizer model data

import pandas as pd
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
train_df["clean_text"] = train_df["translated_text"].apply(preprocess_text)
dev_df["clean_text"] = dev_df["translated_text"].apply(preprocess_text)

train_df.to_csv("preprocessed_train.csv", index=False)
dev_df.to_csv("preprocessed_dev.csv", index=False)

train=pd.read_csv("preprocessed_train.csv")

train

from google.colab import files
files.download("preprocessed_train.csv")
files.download("preprocessed_dev.csv")

import matplotlib.pyplot as plt

plt.figure(figsize=(8,4))
train_df["label"].value_counts().plot(kind="bar", title="Label Distribution After Preprocessing")
plt.show()

# Keep only relevant columns
train_df = train_df[['translated_text', 'label']].dropna()
dev_df = dev_df[['translated_text', 'label']].dropna()

# Check class distribution
print("Train Label Distribution:\n", train_df['label'].value_counts())
print("Dev Label Distribution:\n", dev_df['label'].value_counts())

# Display sample after preprocessing
display(train_df.head(), dev_df.head())

!pip install transformers datasets

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Load MedBERT tokenizer
model_name = "dmis-lab/biobert-base-cased-v1.1" # MedBERT model from Hugging Face
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Tokenize the dataset
def tokenize_data(data):
    return tokenizer(
        data["translated_text"].tolist(),
        padding=True,
        truncation=True,
        max_length=256,  # Adjust if needed
        return_tensors="pt"
    )

train_encodings = tokenize_data(train_df)
dev_encodings = tokenize_data(dev_df)

# Convert labels to tensors
train_labels = torch.tensor(train_df['label'].values)
dev_labels = torch.tensor(dev_df['label'].values)

# Check tokenized output
print("Example Tokenized Input:", train_encodings['input_ids'][0])

import torch
from torch.utils.data import Dataset, DataLoader

# Create a PyTorch dataset class
class ADE_Dataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}
        item["labels"] = self.labels[idx].clone().detach()
        return item

# Create dataset objects
train_dataset = ADE_Dataset(train_encodings, train_labels)
dev_dataset = ADE_Dataset(dev_encodings, dev_labels)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
dev_loader = DataLoader(dev_dataset, batch_size=16, shuffle=False)

# Check batch shape
batch = next(iter(train_loader))
print({key: value.shape for key, value in batch.items()})

from transformers import AutoModelForSequenceClassification

# Load model (2 classes, adjust num_labels if needed)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

!pip install evaluate

from transformers import TrainingArguments
import evaluate
import numpy as np

# Load accuracy metric
accuracy_metric = evaluate.load("accuracy")

# Define function to compute accuracy
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return accuracy_metric.compute(predictions=predictions, references=labels)

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=10,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_accuracy",  # Fix: Prefix with 'eval_'
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

# Train the model
trainer.train()

results = trainer.evaluate()
print(results)

model.save_pretrained("./biobert_model")
tokenizer.save_pretrained("./biobert_model")

import torch

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Move model to GPU
model.to(device)

# Move encodings and labels to GPU
train_encodings = {key: val.to(device) for key, val in train_encodings.items()}
dev_encodings = {key: val.to(device) for key, val in dev_encodings.items()}
train_labels = train_labels.to(device)
dev_labels = dev_labels.to(device)

# Update dataset class to move batches to GPU
class ADE_Dataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}
        item["labels"] = self.labels[idx].clone().detach()
        return item

# Create DataLoaders
train_dataset = ADE_Dataset(train_encodings, train_labels)
dev_dataset = ADE_Dataset(dev_encodings, dev_labels)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
dev_loader = DataLoader(dev_dataset, batch_size=16, shuffle=False)

# Move DataLoader batches to GPU during training
for batch in train_loader:
    batch = {key: val.to(device) for key, val in batch.items()}
    print({key: value.device for key, value in batch.items()})  # Check if everything is on GPU
    break  # Just checking one batch

import matplotlib.pyplot as plt

# Extract training history
train_loss = trainer.state.log_history

# Separate loss and accuracy values
epochs = []
train_losses = []
eval_losses = []
eval_accuracies = []

for log in train_loss:
    if "loss" in log:
        train_losses.append(log["loss"])
    if "epoch" in log:
        epochs.append(log["epoch"])
    if "eval_loss" in log:
        eval_losses.append(log["eval_loss"])
    if "eval_accuracy" in log:
        eval_accuracies.append(log["eval_accuracy"])

# Plot Training vs Validation Loss
plt.figure(figsize=(10, 5))
plt.plot(epochs[:len(train_losses)], train_losses, label="Training Loss", marker="o")
plt.plot(epochs[:len(eval_losses)], eval_losses, label="Validation Loss", marker="o")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid()
plt.show()

# Plot Accuracy (if available)
if eval_accuracies:
    plt.figure(figsize=(10, 5))
    plt.plot(epochs[:len(eval_accuracies)], eval_accuracies, label="Validation Accuracy", marker="o", color="green")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.title("Validation Accuracy Over Epochs")
    plt.legend()
    plt.grid()
    plt.show()

# Save the model and tokenizer
model.save_pretrained("saved_model")
tokenizer.save_pretrained("saved_model")

# Instead of:
# model.save("my_model.h5")  # HDF5 format

# Use this:
model.save_pretrained("my_model") # Saves the model to a directory named "my_model"
tokenizer.save_pretrained("my_model") # Save tokenizer to same directory

!pip install transformers

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import pandas as pd
from tqdm import tqdm

# Load tokenizer and model (PyTorch)
tokenizer = AutoTokenizer.from_pretrained("my_model")
model = AutoModelForSequenceClassification.from_pretrained("my_model")
model.eval()  # Set to evaluation mode

# Load test data
test_df = pd.read_csv("/content/drive/MyDrive/Collab Dataset/Drug Detection/test_data_SMM4H_2025_Task_1_no_labels.csv")

# Prepare batch predictions
predictions = []
batch_size = 32

with torch.no_grad():
    for i in tqdm(range(0, len(test_df), batch_size)):
        texts = test_df['text'][i:i+batch_size].tolist()
        encodings = tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=128)

        outputs = model(**encodings)
        logits = outputs.logits
        batch_preds = torch.argmax(logits, dim=1).tolist()
        predictions.extend(batch_preds)

# Add predictions to dataframe and save
test_df['predicted_label'] = predictions
final_df = test_df[['id', 'predicted_label']]
final_df.to_csv("submission.csv", index=False)

